把没有标注的数据利用起来！不浪费未标注数据，是算法工程师的一种“必备技能”！
低资源少样本NLP问题是JayJay比较关注的一个方向，说白了就是一个问题：标注样本少怎么办？而半监督学习就是解决这一问题的一个重要手段。

半监督学习在CV领域早已经“大显身手”，而在NLP领域的应用却不太多。此外，随着BERT等预训练模型的强大，本质上也缓解了少样本问题。而BERT如果能和其他机器学习方法（如：半监督学习 或 主动学习 等）结合起来，或许少样本问题的增益会更加明显。

本文JayJay介绍一篇来自ACL20的paper《MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification》。
论文下载地址：https://arxiv.org/pdf/2004.12239
论文开源地址：https://github.com/GT-SALT/MixText
MixText主要针对的是半监督文本分类场景，其主要的亮点有：
提出一种全新文本增强方式——TMix，在隐式空间插值，生成全新样本。
对未标注样本进行低熵预测，并与标注样本混合进行TMix。MixText可以挖掘句子之间的隐式关系，并在学习标注样本的同时利用无标注样本的信息。
超越预训练模型和其他半监督方法， 在少样本场景下表现卓越！
本文的组织结构为：

图片
1、回顾：半监督文本分类有哪些方法？

数据为王，数据是深度学习时代的“煤油电”。虽然标注数据获取昂贵，但半监督学习可以同时标注数据和未标注数据，而未标注数据通常很容易得到。

总的看，半监督文本分类可分为以下4种：

变分自编码VAE：通过重构句子，并使用从重构中学到的潜在变量来预测句子标签；
自训练：通过self-training的方式，让模型在未标注数据上生成高置信度的标签；
一致性训练：通过 对抗噪声 或者 数据增强 的方式对未标注数据进行一致性训练；
微调预训练模型：在大规模无标注数据上进行预训练，在下游标注数据上微调；
也许你会问，一致性训练是啥？我们这里补充介绍2种半监督深度学习利用未标注数据的训练方式：

1、熵最小化：根据半监督学习的Cluster假设，决策边界应该尽可能地通过数据较为稀疏的地方（低密度区），以能够避免把密集的样本数据点分到决策边界的两侧。也就是模型通过对未标记数据预测后要作出低熵预测，即熵最小化：


2、一致性训练：对于未标记数据，希望模型在其输入受到扰动时产生相同的输出分布。即：

上述介绍的4种半监督文本分类方式有一个不足之处：就是分开利用标注数据和未标注数据，没有在二者之间直接建立联系。大多数半监督模型仍然会在小标注样本上过拟合。

2、TMix：新颖的文本数据增强方式

图片
Mixup是图像领域常见的一种数据增强方式，其方式非常简单，如上图所示：可直接在像素级别上进行插值，公式如下(其中为图像输入，为标签one-hot向量)：



由于文本的输入是离散的，因此不能直接输入层直接进行Mixup。所以作者提出了一种在隐空间进行插值的方法——TMix，如下图所示。

图片
TMix理解起来也是相当简单，其在BERT编码层进行隐空间插值，相比于直接在输入层进行Mixup，TMix的数据增强的空间范围更加广阔。

需要特别指出的是，从Beta分布进行采样得到：图片

3、MixText的计算流程

图片
MixText的计算流程如上图所示，其共分为三步：

第1步：对未标注数据进行增强：采用未标注数据采取回译方式进行次增强生成;

第2步：对未标注数据进行标签预测：将原始未标注数据和增强后的未标注数据一同喂入到当前模型中，通过平均加权的方式对未标注数据进行预测：

图片
由于预测出的“伪标签”分布相对平坦，论文也采取了Sharpen操作：使得“伪”标签熵更低，即猫狗分类中，要么百分之九十多是猫，要么百分之九十多是狗。Sharpen操作在CV中也应用很广泛啦：


第3步：对标注数据和未标注数据一同进行TMix：将所有数据（有标注，原始未标注，增强后的未标注）混合在一起生成,然后随机选择2个样本和进行TMix，然后通过KL散度计算损失:

图片
当来源不同时，上述损失代表不同的意义：

当x来自标注数据时，主要利用信息来自有标注数据，因此模型损失为有监督损失。
当x来自未标注数据时，主要利用信息来自未标注数据，因此模型损失为一致性训练损失。
JayJay的一个小疑问：主要信息来源于 标注数据 还是 未标注数据，其实主要应该取决于TMix的值是多少，论文这里描述似乎不太恰当。
值得注意的是：在CV领域论文MixMatch[1]中，数值大部分落在0或1附近，因此其主要利用来自而不是。

4、MixText表现如何？

图片
论文最后在4个数据集上，将MixText与BERT和其他半监督方法进行了对比（如上图），可以看出：

MixText在各个数据集上超越了之前的半监督SOTA——谷歌的UDA方法；
MixText在不同标注数据量均取得最高指标，标注数据越少，指标增益越明显；
仅仅利用标注数据的TMix方法也超越了BERT。
图片
此外，作者还对比在仅有10条标注数据时、利用不同的未标注数据量时MixText的表现，如上图所示：随着未标注数据的增加，指标也不断提升！
